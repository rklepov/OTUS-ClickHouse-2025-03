# [16] Загрузка данных в ClickHouse

В данной работе я буду решать задачу переноса данных из БД *PostgreSQL* в БД *ClickHouse* через ETL пайплайн. В качестве DI инструмента я выбрал [*Airbyte*](https://github.com/airbytehq/airbyte).

> Очевидно, что подобную задачу можно было бы проще решить с помощью [соответствующего](https://clickhouse.com/docs/integrations/postgresql#using-the-postgresql-table-engine "Connecting ClickHouse to PostgreSQL") интеграционного движка *ClickHouse* (эта тема подробнее рассматривается в задании [*[19] Интеграция ClickHouse с PostgreSQL*](https://github.com/rklepov/OTUS-ClickHouse-2025-03/wiki/%5B16%5D-%D0%97%D0%B0%D0%B3%D1%80%D1%83%D0%B7%D0%BA%D0%B0-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B2-ClickHouse)). Однако в этом задании нашей целью является исследование DI инструментов и настройка конвейера ETL.

ETL конвейер для этой задачи концептуально довольно простой. Но, я думаю, что для учебной работы этого вполне достаточно. Кроме того, настройка *Airbyte* сама по себе оказалась довольно трудоёмкой задачей.

![Airbyte connections][airbyte_connections_logo]

[airbyte_connections_logo]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-25_DI/hw16/img/airbyte_connections_logo.png

## Архитектура решения

Я буду запускать процессы источника *PostgreSQL* и получателя *ClickHouse* в *Docker* контейнерах на виртуальной машине *Linux*, потому что у меня уже есть подготовленная инфраструктура для этого с предыдущих заданий (традиционно на этом курсе я работаю в Linux [CentOS 9](https://www.centos.org/stream9/) локально через [VirtualBox](https://www.virtualbox.org/)).

Однако сам *Airbyte* требует для запуска [*Docker Desktop*](https://docs.docker.com/desktop/). Поскольку *Docker Desktop* невозможно запустить внутри виртуальной машины, то я был вынужден установить его непосредственно на своей хост-машине *Windows*. *Airbyte* будет взаимодействовать с серверами БД источника и получателя через их внешние порты (порты серверов предварительно необходимо "прокинуть" через несколько уровней: наружу из контейнера, затем открыть в *firewall* Linux, а потом наружу из виртуальной машины на хост-машину).

Итак, серверы *PostgreSQL* и *ClickHouse* работают в контейнерах на виртуальной машине *Linux*, а *Airbyte* работает в контейнере, который запущен на [WSL](https://learn.microsoft.com/en-us/windows/wsl/ "Windows Subsystem for Linux").

> Да, получается сложновато, но, с другой стороны, таким образом мы даже более явно моделируем ситуацию с внешними по отношению к *Airbyte* источником и получателем.

## Источник и получатель

Контейнеры *PostgreSQL* и *ClickHouse* описаны в общем файле `docker-compose.yaml` и запускаются одновременно.

<details>

<summary>docker-compose.yaml</summary>

```yaml
version: "3.8"

services:
  postgres-src:
    image: postgres:latest
    command: -c config_file=/etc/postgresql/postgresql.conf
    restart: always
    hostname: src-pg
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - "0.0.0.0:15432:5432"
    volumes:
      - src_db:/var/lib/postgresql/data
      - ${PWD}/fs/volumes/postgres/import:/import:Z
      - ${PWD}/fs/volumes/postgres/etc/postgresql/postgresql.conf:/etc/postgresql/postgresql.conf:Z

  clickhouse-dst:
    image: "clickhouse/clickhouse-server:latest"
    user: "0:0"
    hostname: dst-ch
    volumes:
      - dst_db:/var/lib/clickhouse
      - ${PWD}/fs/volumes/clickhouse/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml:Z
      - ${PWD}/fs/volumes/clickhouse/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml:Z
    ports:
      - "0.0.0.0:18123:8123"
      - "0.0.0.0:19000:9000"

volumes:
  src_db:
    driver: local
  dst_db:
    driver: local
```

</details>

Конфигурационные файлы серверов *PostgreSQL* и *ClickHouse* хранятся вне контейнеров и подключаются через соответствующие *volumes*, описанные в `docker-compose.yaml`. Все файлы конфигурации можно найти в текущем [репозитории](https://github.com/rklepov/OTUS-ClickHouse-2025-03/tree/main/2025-06-25_DI/hw16).

Запустим контейнеры с серверами *PostgreSQL* и *ClickHouse* помощью команды `docker compose up -d`.

```shell
$ docker ps

CONTAINER ID  IMAGE                                          COMMAND     CREATED         STATUS         PORTS                                                       NAMES
ecb2a7eee708  docker.io/library/postgres:latest              postgres    24 minutes ago  Up 24 minutes  0.0.0.0:15432->5432/tcp                                     hw16_postgres-src_1
f072712da106  docker.io/clickhouse/clickhouse-server:latest              24 minutes ago  Up 24 minutes  0.0.0.0:18123->8123/tcp, 0.0.0.0:19000->9000/tcp, 9009/tcp  hw16_clickhouse-dst_1
```

### Данные источника (*PostgreSQL*)

Я буду использовать демонстрационную базу данных [&laquo;Авиаперевозки&raquo;](https://postgrespro.ru/education/demodb), предоставляемую компанией [PostgresPro](https://postgrespro.ru/).

Загрузим архив с данными по полётам за один месяц и распакуем его в директорию, доступную из контейнера *PostgreSQL*.

```shell
$ wget -q --no-check-certificate https://edu.postgrespro.ru/demo-small.zip

$ unzip demo-small.zip -d fs/volumes/postgres/import/
Archive:  demo-small.zip
  inflating: fs/volumes/postgres/import/demo-small-20170815.sql

$ ls -hAlF fs/volumes/postgres/import/
total 100M
-rw-rw-r--. 1 rklepov rklepov 100M Aug 22  2017 demo-small-20170815.sql
```

Зайдём в контейнер *PostgreSQL* и создадим БД с помощью утилиты **psql** в соответствии с [инструкцией](https://postgrespro.ru/docs/postgrespro/15/demodb-bookings-installation).

```shell
$ docker exec -it hw16_postgres-src_1 bash

root@src-pg:/# psql -f import/demo-small-20170815.sql -U postgres

. . . . .

COPY 9
COPY 104
COPY 579686
COPY 262788
COPY 33121
 setval
--------
  33121
(1 row)

COPY 1339
COPY 1045726
COPY 366733

. . . . .
```

Проверим состояние созданной БД:

```text
root@src-pg:/# psql demo -U postgres
psql (17.5 (Debian 17.5-1.pgdg120+1))
Type "help" for help.

demo=# \set PROMPT1 '%/=> '

demo=> \d
                   List of relations
  Schema  |         Name          |   Type   |  Owner
----------+-----------------------+----------+----------
 bookings | aircrafts             | view     | postgres
 bookings | aircrafts_data        | table    | postgres
 bookings | airports              | view     | postgres
 bookings | airports_data         | table    | postgres
 bookings | boarding_passes       | table    | postgres
 bookings | bookings              | table    | postgres
 bookings | flights               | table    | postgres
 bookings | flights_flight_id_seq | sequence | postgres
 bookings | flights_v             | view     | postgres
 bookings | routes                | view     | postgres
 bookings | seats                 | table    | postgres
 bookings | ticket_flights        | table    | postgres
 bookings | tickets               | table    | postgres
(13 rows)
```

Для примера посмотрим, как выглядят данные по аэропортам при запросе через *view* **airports**:

```sql
demo=> select count(*) from airports;
 count
-------
   104
(1 row)

demo=> select * from airports limit 10;

 airport_code |  airport_name   |           city           |               coordinates               |      timezone
--------------+-----------------+--------------------------+-----------------------------------------+--------------------
 YKS          | Якутск          | Якутск                   | (129.77099609375,62.093299865722656)    | Asia/Yakutsk
 MJZ          | Мирный          | Мирный                   | (114.03900146484375,62.534698486328125) | Asia/Yakutsk
 KHV          | Хабаровск-Новый | Хабаровск                | (135.18800354004,48.52799987793)        | Asia/Vladivostok
 PKC          | Елизово         | Петропавловск-Камчатский | (158.45399475097656,53.16790008544922)  | Asia/Kamchatka
 UUS          | Хомутово        | Южно-Сахалинск           | (142.71800231933594,46.88869857788086)  | Asia/Sakhalin
 VVO          | Владивосток     | Владивосток              | (132.1479949951172,43.39899826049805)   | Asia/Vladivostok
 LED          | Пулково         | Санкт-Петербург          | (30.262500762939453,59.80030059814453)  | Europe/Moscow
 KGD          | Храброво        | Калининград              | (20.592599868774414,54.88999938964844)  | Europe/Kaliningrad
 KEJ          | Кемерово        | Кемерово                 | (86.1072006225586,55.27009963989258)    | Asia/Novokuznetsk
 CEK          | Челябинск       | Челябинск                | (61.5033,55.305801)                     | Asia/Yekaterinburg
(10 rows)
```

### Получатель (*ClickHouse*)

Сервер *ClickHouse* запускается обычным образом. Покажем, что изначально данные в БД получателя отсутствуют.

```sql
SHOW DATABASES

   ┌─name───────────────┐
1. │ INFORMATION_SCHEMA │
2. │ default            │
3. │ information_schema │
4. │ system             │
   └────────────────────┘

4 rows in set. Elapsed: 0.006 sec.


SELECT *
FROM system.tables
WHERE lower(database) NOT IN ('default', 'system', 'information_schema')

Ok.

0 rows in set. Elapsed: 0.013 sec.
```

## Настройка *Airbyte*

### Установка *Airbyte*

Проведём установку  в соответствии с [инструкцией](https://docs.airbyte.com/platform/using-airbyte/getting-started/oss-quickstart). Как уже было отмечено [выше](#архитектура-решения), предварительно пришлось также установить *Docker Desktop*.

Установка и запуск *Airbyte* выполняются с помощью утилиты **abctl**: `abctl local install` (утилиту **abctl** тоже необходимо сначала установить, как описано в *п.2* инструкции). Эта команда скачает *~20Gb* образов контейнеров, и стартует один из контейнеров, в котором в том числе работает HTTP сервер, по умолчанию доступный по адресу <http://localhost:8000>. Через его web-интерфейс и происходит дальнейшее взаимодействие с *Airbyte*.

> В моём случае команда `abctl local install` при первом запуске выполнялась примерно *40 мин.*

После завершения `abctl local install` проверим итоговое состояние установки *Airbyte*.

```text
PS C:\Users\rklepov> abctl local deployments

  INFO    Using Kubernetes provider:
            Provider: kind
            Kubeconfig: C:\Users\rklepov\.airbyte\abctl\abctl.kubeconfig
            Context: kind-airbyte-abctl
 SUCCESS  Found Docker installation: version 28.3.0
  INFO    Found the following deployments:
            airbyte-abctl-connector-builder-server
            airbyte-abctl-cron
            airbyte-abctl-server
            airbyte-abctl-temporal
            airbyte-abctl-webapp
            airbyte-abctl-worker
            airbyte-abctl-workload-api-server
            airbyte-abctl-workload-launcher
```

А вот список образов контейнеров, которые были скачаны в процессе установки *Airbyte*:

```text
PS C:\Users\rklepov> docker image ls

REPOSITORY                         TAG        IMAGE ID       CREATED        SIZE
airbyte/connector-builder-server   1.7.1      68e686d93170   2 weeks ago    4.85GB
airbyte/workload-api-server        1.7.1      a20516b7df87   2 weeks ago    1.18GB
airbyte/cron                       1.7.1      67677d273914   2 weeks ago    1.25GB
airbyte/connector-sidecar          1.7.1      4197bad38551   2 weeks ago    1.27GB
airbyte/server                     1.7.1      cd5429d46952   2 weeks ago    1.31GB
airbyte/workload-launcher          1.7.1      87d7bd75122b   2 weeks ago    1.27GB
airbyte/bootloader                 1.7.1      26cf1f249e0d   2 weeks ago    1.23GB
airbyte/container-orchestrator     1.7.1      4f6f4d2f0a4f   2 weeks ago    1.26GB
airbyte/worker                     1.7.1      dc743fc2b65c   2 weeks ago    1.28GB
airbyte/webapp                     1.7.1      cffb6104eae1   2 weeks ago    38.2MB
airbyte/db                         1.7.0-17   627a8b880e52   2 weeks ago    398MB
airbyte/airbyte-base-java-image    3.3.6      b98f468f048a   4 weeks ago    877MB
temporalio/auto-setup              1.27.2     b44cbfeb43db   3 months ago   680MB
kindest/node                       <none>     f226345927d7   4 months ago   1.49GB
```

Интересно, что при этом исполняется только один контейнер:

```text
PS C:\Users\rklepov> docker ps

CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                             NAMES
2663bdd47aea   kindest/node:v1.32.2   "/usr/local/bin/entr…"   5 hours ago   Up 5 hours   0.0.0.0:8000->80/tcp, 127.0.0.1:51200->6443/tcp   airbyte-abctl-control-plane
```

Как указано в [документации](https://github.com/airbytehq/abctl/blob/main/README.md), *Airbyte* работает на кластере *Kubernetes*, который при локальной установке запускается внутри контейнера *Docker* :

> *Airbyte* platform requires a [*Kubernetes*](https://kubernetes.io/) cluster, which `abctl` creates by utilizing [*kind*](https://kind.sigs.k8s.io/) (kind runs a *Kubernetes* cluster within a *Docker* container).

Наконец, посмотрим на текущий статус *Airbyte*.

```text
PS C:\Users\rklepov> abctl local status

  INFO    Using Kubernetes provider:
            Provider: kind
            Kubeconfig: C:\Users\rklepov\.airbyte\abctl\abctl.kubeconfig
            Context: kind-airbyte-abctl
 SUCCESS  Found Docker installation: version 28.3.0
 SUCCESS  Existing cluster 'airbyte-abctl' found
  INFO    Found helm chart 'airbyte-abctl'
            Status: deployed
            Chart Version: 1.7.1
            App Version: 1.7.1
  INFO    Found helm chart 'ingress-nginx'
            Status: deployed
            Chart Version: 4.13.0
            App Version: 1.13.0
  INFO    Airbyte should be accessible via http://localhost:8000
```

### Настройка источника (`Source`)

Чтобы использовать коннектор *Postgres* в *Airbyte* в режиме *Change Data Capture (CDC)* необходимо настроить [*логическую репликацию*](https://postgrespro.ru/docs/postgrespro/current/logicaldecoding) в БД *PostgreSQL*. Моих текущих знаний *PostgreSQL* пока не хватает, чтобы сделать это осознанно, поэтому просто повторяем шаги из [инструкции](https://docs.airbyte.com/integrations/sources/postgres#advanced-configuration-using-cdc) по коннектору *Postgres*, не вдаваясь в суть выполняемых действий.

Добавляем права пользователю.

```sql
demo=> ALTER USER postgres REPLICATION;
ALTER ROLE
```

Включаем *логическую репликацию* в конфигурации сервера *PostgreSQL*, [изменив](https://github.com/rklepov/OTUS-ClickHouse-2025-03/commit/0cdf32841e7a39f7f0fefaa5c05293ec66b04f9a) настройки в соответствии с *п.3* инструкции.

Создаём *слот репликации*.

```sql
demo=> SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');
 pg_create_logical_replication_slot
------------------------------------
 (airbyte_slot,0/29EE74D0)
(1 row)

demo=> SELECT slot_name, plugin, slot_type, database from  pg_replication_slots;
  slot_name   |  plugin  | slot_type | database
--------------+----------+-----------+----------
 airbyte_slot | pgoutput | logical   | demo
(1 row)
```

Создаём *идентификации реплики* для каждой таблицы.

```sql
ALTER TABLE aircrafts_data  REPLICA IDENTITY DEFAULT;
ALTER TABLE airports_data   REPLICA IDENTITY DEFAULT;
ALTER TABLE boarding_passes REPLICA IDENTITY DEFAULT;
ALTER TABLE bookings        REPLICA IDENTITY DEFAULT;
ALTER TABLE flights         REPLICA IDENTITY DEFAULT;
ALTER TABLE seats           REPLICA IDENTITY DEFAULT;
ALTER TABLE ticket_flights  REPLICA IDENTITY DEFAULT;
ALTER TABLE tickets         REPLICA IDENTITY DEFAULT;
```

Создаём *публикацию* для таблиц.

```sql
demo=> CREATE PUBLICATION airbyte_publication FOR TABLE aircrafts_data, airports_data, boarding_passes, bookings, flights, seats, ticket_flights, tickets;
CREATE PUBLICATION
```

Наконец, создаём коннектор источника *Postgres* через web-интерфейс *Airbyte*. Параметры коннектора можно извлечь в формате JSON:

```json
{
  "name": "Postgres",
  "workspaceId": "ce1b0b64-30ba-4153-822e-c034fc962b09",
  "definitionId": "decd338e-5647-4c0b-adf4-da0e75f5a750",
  "configuration": {
    "host": "host.docker.internal",
    "port": 15432,
    "schemas": [
      "bookings"
    ],
    "database": "demo",
    "password": "******",
    "ssl_mode": {
      "mode": "disable"
    },
    "username": "postgres",
    "tunnel_method": {
      "tunnel_method": "NO_TUNNEL"
    },
    "replication_method": {
      "method": "CDC",
      "plugin": "pgoutput",
      "queue_size": 10000,
      "publication": "airbyte_publication",
      "replication_slot": "airbyte_slot",
      "lsn_commit_behaviour": "After loading Data in the destination",
      "initial_waiting_seconds": 1200,
      "initial_load_timeout_hours": 8,
      "invalid_cdc_cursor_position_behavior": "Fail sync"
    }
  }
}
```

### Настройка получателя (`Destination`)

В то время как коннектор *Postgres* идёт в поставке *Airbyte*, коннектор *Clickhouse*, очевидно, развивается сторонними разработчиками, и поэтому доступен в разделе *Marketplace*. Настраиваем его в соответствии с [инструкцией](https://docs.airbyte.com/integrations/destinations/clickhouse). Здесь каких-то особенностей нет. Параметры коннектора в формате JSON:

```json
{
  "name": "Clickhouse",
  "workspaceId": "ce1b0b64-30ba-4153-822e-c034fc962b09",
  "definitionId": "ce0d828e-1dc4-496c-b122-2da42e637e48",
  "configuration": {
    "host": "host.docker.internal",
    "port": "18123",
    "database": "default",
    "password": "******",
    "protocol": "**http**",
    "username": "default",
    "enable_json": true
  }
}
```

### Связь источника и получателя (`Connection`)

Следующим действием необходимо связать источник данных с получателем. В web-интерфейсе *Airbyte* созданное соединение выглядит вот так:

![Postgres -> Clickhouse connection][airbyte_pg-ch_connection]

[airbyte_pg-ch_connection]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-25_DI/hw16/img/airbyte_pg-ch_connection.png

## Выгрузка данных

> Сразу хочу отметить, что целиком БД **bookings** мне так и не удалось перенести: в моём случае даже на таком небольшом объёме данных *Airbyte* постоянно зависал и отваливался. Поэтому, чтобы продемонстрировать хотя бы какой-то результат, ниже я ограничился переносом только нескольких самых маленьких таблиц. При этом даже эти данные (а это меньше 200 строк), копировались почти 30 минут.

### Синхронизация источника и получателя

Сначала необходимо настроить схему отображения данных источника на таблицы получателя. Здесь мы выбираем набор таблиц и их полей, настраиваем тип синхронизации (полное или инкрементальное обновление) и отдельные параметры для получателя (напр. первичный ключ таблицы). Также можно настроить расписание обновления.

![Airbyte connection schema][airbyte_connection_schema]

[airbyte_connection_schema]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-25_DI/hw16/img/airbyte_connection_schema.png

В данном случае я выбрал однократное ручное обновление (тип расписания: *manual*). Далее стартуем синхронизацию.

![Airbyte sync progress][airbyte_synс_progress]

[airbyte_synс_progress]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-25_DI/hw16/img/airbyte_synс_progress.png

Синхронизация занимает некоторое время. После окончания переноса данных статус выглядит так:

![Airbyte sync completed][airbyte_sync_completed]

[airbyte_sync_completed]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-25_DI/hw16/img/airbyte_sync_completed.png

### Состояние целевой БД

Посмотрим, как изменилось состояние *ClickHouse* и как выглядят загруженные данные.

Во-первых, была создана БД **bookings**, которая соответствует одноимённой схеме в БД *PostgreSQL*.

```sql
SHOW DATABASES

   ┌─name───────────────┐
1. │ INFORMATION_SCHEMA │
2. │ bookings           │
3. │ default            │
4. │ information_schema │
5. │ system             │
   └────────────────────┘

5 rows in set. Elapsed: 0.103 sec.
```

Во-вторых, в этой БД были созданы и заполнены данными таблицы:

```sql
SELECT
    database,
    name,
    engine,
    total_rows,
    formatReadableSize(total_bytes) AS size,
    parts
FROM system.tables
WHERE database IN ('bookings')

   ┌─database─┬─name───────────┬─engine─────────────┬─total_rows─┬─size──────┬─parts─┐
1. │ bookings │ aircrafts      │ ReplacingMergeTree │          9 │ 1.38 KiB  │     1 │
2. │ bookings │ aircrafts_data │ ReplacingMergeTree │          9 │ 1.54 KiB  │     1 │
3. │ bookings │ airports       │ ReplacingMergeTree │        104 │ 10.46 KiB │     1 │
4. │ bookings │ airports_data  │ ReplacingMergeTree │        104 │ 13.02 KiB │     1 │
   └──────────┴────────────────┴────────────────────┴────────────┴───────────┴───────┘

4 rows in set. Elapsed: 0.028 sec.
```

Как можно видеть, это именно те 4 таблицы, для которых мы и настроили синхронизацию в *Airbyte* [выше](#связь-источника-и-получателя-connection). Также можно обратить внимание на движок, который был выбран коннектором *ClickHouse* для создаваемых таблиц: это [*ReplacingMergeTree*](https://clickhouse.com/docs/engines/table-engines/mergetree-family/replacingmergetree).

Посмотрим на структуру одной из таблиц.

```sql
SHOW CREATE TABLE bookings.airports
FORMAT Raw

CREATE TABLE bookings.airports
(
    `_airbyte_raw_id` String,
    `_airbyte_extracted_at` DateTime64(3),
    `_airbyte_meta` String,
    `_airbyte_generation_id` UInt32,
    `city` Nullable(String),
    `timezone` Nullable(String),
    `_ab_cdc_lsn` Nullable(Decimal(38, 9)),
    `coordinates` Nullable(String),
    `airport_code` String,
    `airport_name` Nullable(String),
    `_ab_cdc_deleted_at` Nullable(String),
    `_ab_cdc_updated_at` Nullable(String)
)
ENGINE = ReplacingMergeTree
ORDER BY airport_code
SETTINGS index_granularity = 8192
```

Можно увидеть, что, помимо полей, соответствующих одноимённым полям в таблице источника, также создаётся некоторое количество служебных колонок (их название начинается с `_`), которые, очевидно, используются для отслеживания состояния синхронизации в режиме CDC.

> Почему в созданной таблице *ClickHouse* именно такой порядок колонок, я пока затрудняюсь ответить. Вероятно, что он произвольный.

Наконец, посмотрим на загруженные данные. Сделаем запрос к таблице **bookings.airports**, аналогичный запросу к *view* **airports** из *PostgreSQL* [выше](#данные-источника-postgresql). Для исключения служебных полей используем функцию [динамического формирования списка колонок](https://clickhouse.com/docs/guides/developer/dynamic-column-selection).

```sql
SELECT COLUMNS('.*') EXCEPT '^_.*'
FROM bookings.airports
LIMIT 10

    ┌─city─────────┬─timezone─────────┬─coordinates────────────────────────────┬─airport_code─┬─airport_name─┐
 1. │ Анапа        │ Europe/Moscow    │ (37.347301483154,45.002101898193)      │ AAQ          │ Витязево     │
 2. │ Абакан       │ Asia/Krasnoyarsk │ (91.38500213623047,53.7400016784668)   │ ABA          │ Абакан       │
 3. │ Сочи         │ Europe/Moscow    │ (39.956600189209,43.449901580811)      │ AER          │ Сочи         │
 4. │ Архангельск  │ Europe/Moscow    │ (40.71670150756836,64.60030364990234)  │ ARH          │ Талаги       │
 5. │ Астрахань    │ Europe/Samara    │ (48.0063018799,46.2832984924)          │ ASF          │ Астрахань    │
 6. │ Барнаул      │ Asia/Krasnoyarsk │ (83.53849792480469,53.363800048828125) │ BAX          │ Барнаул      │
 7. │ Благовещенск │ Asia/Yakutsk     │ (127.41200256347656,50.42539978027344) │ BQS          │ Игнатьево    │
 8. │ Братск       │ Asia/Irkutsk     │ (101.697998046875,56.370601654052734)  │ BTK          │ Братск       │
 9. │ Брянск       │ Europe/Moscow    │ (34.176399231,53.214199066199996)      │ BZK          │ Брянск       │
10. │ Череповец    │ Europe/Moscow    │ (38.015800476100004,59.273601532)      │ CEE          │ Череповец    │
    └──────────────┴──────────────────┴────────────────────────────────────────┴──────────────┴──────────────┘

10 rows in set. Elapsed: 0.037 sec.
```

Для таблицы **airports** в *ClickHouse* настроен первичный ключ по полю *airport_code*, поэтому данные упорядочены соответствующим образом.

## Заключение

В целом хочу отметить, что решение *Airbyte* мне не очень понравилось. Во-первых, оно слишком громоздкое. Установка требует *Docker Desktop* и занимает несколько десятков гигабайт на диске. При этом в процессе переноса данных расходуется дополнительное место на диске, по какой-то причине тоже измеряемое гигабайтами, что не пропорционально реальному содержимому БД источника. Во-вторых, решение оказалось не очень надёжным. Даже на небольшом объёме данных из моего примера мне так и не удалось перенести всю базу целиком, потому что при попытке сделать это, постоянно что-то отваливалось или зависало. Наконец, хотя я и понимаю, что основная идея *Airbyte* состоит в том, что пользователь имеет возможность связывать разные источники и получатели данных с помощью удобного визуального интерфейса, но, если хотя бы что-то при этом идёт не так, то возникающие ошибки довольно сложно диагностировать (а в недра кластера *Kubernetes*, работающего внутри контейнера *Docker*, доберётся далеко не каждый). При этом, как я уже отметил, эти ошибки происходят постоянно. В итоге, чтобы только продемонстрировать мой небольшой пример, *Airbyte* пришлось переустанавливать несколько раз.

> Постфактум я понял, что, возможно, надо было инсталлировать *Airbyte* в режиме *low-resource mode*: `abctl local install --low-resource-mode`. Хотя я не ожидал, что это мой вариант, ведь в [документации](https://docs.airbyte.com/platform/using-airbyte/getting-started/oss-quickstart#part-3-run-airbyte) рекомендуется использовать этот режим для случая *"fewer than 4 CPUs"*.

Тем не менее цель учебного задания можно считать достигнутой: изучен DI инструмент *Airbyte*, настроен пайплайн ETL, с помощью которого некоторые данные успешно перенесены из *PostgreSQL* в *ClickHouse*.
