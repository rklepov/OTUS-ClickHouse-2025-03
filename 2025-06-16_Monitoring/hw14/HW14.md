# [14] Мониторинг

> Я работаю локально через [VirtualBox](https://www.virtualbox.org/) на Linux [CentOS 9](https://www.centos.org/stream9/).

## Архитектура решения

Поскольку в [третьей части](#репликация-логов-) работы (задание со звездочкой \*) мы будем настраивать репликацию логов, то я буду использовать уже готовое решение из домашнего задания [*[09] Репликация и удаление*](/rklepov/OTUS-ClickHouse-2025-03/wiki/%5B09%5D-%D0%A0%D0%B5%D0%BF%D0%BB%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-%D1%83%D0%B4%D0%B0%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5), где уже настроен кластер *ClickHouse* с двумя репликами.

Также сразу добавим в `docker-compose.yaml` настройки для контейнеров [*Prometheus*](https://prometheus.io/docs/prometheus/latest/getting_started/) и [*Grafana*](https://grafana.com/docs/). Здесь я основывался на примерах из статьи [**(2)**](#biblio-02).

В итоге у нас запускается 8 контейнеров: 2 реплики *ClickHouse*, 3 экземпляра *ClickHouse Keeper* (для кворума), *Prometheus*, [*Node Exporter*](https://prometheus.io/docs/guides/node-exporter/) (чтобы можно было сразу посмотреть на системные метрики) и *Grafana*.

<details>

<summary>docker-compose.yaml</summary>

```yaml
version: "3.8"

services:
  clickhouse-01:
    image: "clickhouse/clickhouse-server:${CHVER:-latest}"
    user: "0:0"
    container_name: clickhouse-01
    hostname: clickhouse-01
    volumes:
      - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml:Z
      - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml:Z
    ports:
      - "0.0.0.0:18123:8123"
      - "0.0.0.0:19000:9000"
    networks:
      - hw14_monitoring
    depends_on:
      - clickhouse-keeper-01
      - clickhouse-keeper-02
      - clickhouse-keeper-03
  clickhouse-02:
    image: "clickhouse/clickhouse-server:${CHVER:-latest}"
    user: "0:0"
    container_name: clickhouse-02
    hostname: clickhouse-02
    volumes:
      - ${PWD}/fs/volumes/clickhouse-02/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml:Z
      - ${PWD}/fs/volumes/clickhouse-02/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml:Z
    ports:
      - "0.0.0.0:28123:8123"
      - "0.0.0.0:29000:9000"
    networks:
      - hw14_monitoring
    depends_on:
      - clickhouse-keeper-01
      - clickhouse-keeper-02
      - clickhouse-keeper-03
  clickhouse-keeper-01:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "0:0"
    container_name: clickhouse-keeper-01
    hostname: clickhouse-keeper-01
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-01/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml:Z
    ports:
      - "0.0.0.0:19181:9181"
    networks:
      - hw14_monitoring
  clickhouse-keeper-02:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "0:0"
    container_name: clickhouse-keeper-02
    hostname: clickhouse-keeper-02
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-02/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml:Z
    ports:
      - "0.0.0.0:29181:9181"
    networks:
      - hw14_monitoring
  clickhouse-keeper-03:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "0:0"
    container_name: clickhouse-keeper-03
    hostname: clickhouse-keeper-03
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-03/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml:Z
    ports:
      - "0.0.0.0:39181:9181"
    networks:
      - hw14_monitoring

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ${PWD}/fs/volumes/prometheus:/etc/prometheus:Z
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
    ports:
      - "19090:9090"
    networks:
      - hw14_monitoring
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($|/)"
    ports:
      - "19100:9100"
    networks:
      - hw14_monitoring
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - ${PWD}/fs/volumes/grafana/provisioning:/etc/grafana/provisioning:Z
      - ${PWD}/fs/volumes/grafana/dashboards:/var/lib/grafana/dashboards:Z
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "13000:3000"
    networks:
      - hw14_monitoring
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  prometheus_data: {}
  grafana_data: {}

networks:
  hw14_monitoring:
    driver: bridge
```

</details>

Все файлы конфигурации можно найти в текущем [репозитории](/rklepov/OTUS-ClickHouse-2025-03/tree/main/2025-06-16_Monitoring/hw14/cluster_1S_2R).

Запустим контейнеры с помощью команды `docker compose up -d`.

```shell
$ docker ps

CONTAINER ID  IMAGE                                                 COMMAND               CREATED            STATUS            PORTS                                                       NAMES
3c4196caf265  docker.io/clickhouse/clickhouse-keeper:latest-alpine                        About an hour ago  Up About an hour  0.0.0.0:19181->9181/tcp, 2181/tcp, 10181/tcp, 44444/tcp     clickhouse-keeper-01
ff11356dd6c0  docker.io/clickhouse/clickhouse-keeper:latest-alpine                        About an hour ago  Up About an hour  0.0.0.0:29181->9181/tcp, 2181/tcp, 10181/tcp, 44444/tcp     clickhouse-keeper-02
8275ebe7ca99  docker.io/clickhouse/clickhouse-keeper:latest-alpine                        About an hour ago  Up About an hour  0.0.0.0:39181->9181/tcp, 2181/tcp, 10181/tcp, 44444/tcp     clickhouse-keeper-03
44f8309adfec  docker.io/prom/prometheus:latest                      --config.file=/et...  About an hour ago  Up About an hour  0.0.0.0:19090->9090/tcp                                     prometheus
9bd9500e8785  docker.io/prom/node-exporter:latest                   --path.procfs=/ho...  About an hour ago  Up About an hour  0.0.0.0:19100->9100/tcp                                     node-exporter
6674f34ddcb7  docker.io/grafana/grafana:latest                                            About an hour ago  Up About an hour  0.0.0.0:13000->3000/tcp                                     grafana
15f520dcda0f  docker.io/clickhouse/clickhouse-server:latest                               About an hour ago  Up About an hour  0.0.0.0:18123->8123/tcp, 0.0.0.0:19000->9000/tcp, 9009/tcp  clickhouse-01
3b841f0e19ae  docker.io/clickhouse/clickhouse-server:latest                               About an hour ago  Up About an hour  0.0.0.0:28123->8123/tcp, 0.0.0.0:29000->9000/tcp, 9009/tcp  clickhouse-02
```

## Встроенные дашборды

```sql
SELECT version()
FORMAT Raw

25.6.4.12
```

```sql
CREATE DATABASE hw14
```

Здесь в целом будем использовать подход, описанный на лекции *"Метрики и мониторинг. Логирование"*, который, в свою очередь, основан на статье [**(1)**](#biblio-01).

Однако у меня не хватает фантазии, чтобы придумать какой-то новый дашборд, основанный на системных метриках, поэтому я решил сделать проще: создать искусственный дашборд для уже ставшего классическим датасета [New York Taxi Data](https://clickhouse.com/docs/getting-started/example-datasets/nyc-taxi).

Поскольку датасет содержит исторические данные, то запросы будут привязаны не к [`now()`](https://clickhouse.com/docs/sql-reference/functions/date-time-functions#now), а к последнему моменту времени из таблицы *nyc_taxi.trips_small*.

Создадим собственную таблицу для запросов по образу [`system.dashboards`](https://clickhouse.com/docs/operations/system-tables/dashboards).

```sql
CREATE OR REPLACE TABLE hw14.dashboards AS system.dashboards
ENGINE = MergeTree
ORDER BY tuple()
```

```sql
SHOW CREATE TABLE hw14.dashboards
FORMAT Raw

CREATE TABLE hw14.dashboards
(
    `dashboard` String COMMENT 'The dashboard name.',
    `title` String COMMENT 'The title of a chart.',
    `query` String COMMENT 'The query to obtain data to be displayed.'
)
ENGINE = MergeTree
ORDER BY tuple()
.  .  .  .  .
```

Далее создадим 4 запроса, которые будут визуализироваться на графиках встроенного дашборда *ClickHouse*.

Запросы будут иметь общую структуру. Данные предварительно агрегируются по заданному интервалу (например, 1 минута), затем сглаживаются в скользящем окне (например, размером 1 час). Длительность отображаемой истории также ограничивается заданным интервалом (например, 24 часа). Как уже было сказано выше, этот интервал отсчитывается не от текущего времени, а от максимального времени в колонке **pickup_datetime** таблицы *nyc_taxi.trips_small*.

Результата запроса должен содержать ровно две колонки. Первая должна иметь имя `t` (жёсткое требование), и содержать метку времени в целочисленном формате ([*Unix timestamp*](https://clickhouse.com/docs/sql-reference/data-types/datetime), количество секунд). Время отображается на графике по оси X. Вторая колонка может иметь произвольное имя и содержит числовое значение, которое отображается на графике по оси Y.

Пример такого запроса ниже. Здесь мы смотрим на среднее расстояние поездки за последний час. Аналогичный запрос, текст которого будет добавлен в таблицу *hw14.dashboards*, естественно, будет параметризован (значения интервалов времени передаются через поля web-формы).

```sql
WITH
    60 as aggr_interval_sec,
    24 as history_depth_hr,
    1 AS smoothing_interval_hr,
    t_interval_aggregated AS
    (
        SELECT
            toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND) AS time,
            avg(trip_distance) AS avg_distance
        FROM nyc_taxi.trips_small
        WHERE time >= (max_time - INTERVAL (history_depth_hr + smoothing_interval_hr) HOUR)
        GROUP BY time
        ORDER BY time ASC WITH FILL STEP aggr_interval_sec
    ),
    (
        SELECT max(toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND))
        FROM nyc_taxi.trips_small
    ) AS max_time
SELECT
    time::INT AS t,
    avg(avg_distance) OVER (ORDER BY t ASC RANGE BETWEEN (smoothing_interval_hr * 3600) PRECEDING AND CURRENT ROW) AS moving_avg_trip_distance
FROM t_interval_aggregated
WHERE time >= (max_time - INTERVAL history_depth_hr HOUR)
LIMIT 10

    ┌──────────t─┬─moving_avg_trip_distance─┐
 1. │ 1443571140 │        4.407142909509795 │
 2. │ 1443571200 │        3.400793681364684 │
 3. │ 1443571260 │       3.5116402416652632 │
 4. │ 1443571320 │        4.062063519185084 │
 5. │ 1443571380 │        4.212881571960991 │
 6. │ 1443571440 │        4.131601308842952 │
 7. │ 1443571500 │        4.214043358889812 │
 8. │ 1443571560 │        4.000551826257651 │
 9. │ 1443571620 │       3.9100143222574224 │
10. │ 1443571680 │        3.835572888424739 │
    └────────────┴──────────────────────────┘

10 rows in set. Elapsed: 0.156 sec. Processed 3.04 million rows, 12.31 MB (19.43 million rows/s., 78.72 MB/s.)
Peak memory usage: 141.89 KiB.
```

Добавим 4 записи с параметризованными запросами в таблицу *hw14.dashboards* (повторю, что размер скользящего окна настраивается, в примерах это *1 час*).

1. Общее число перевезённых за последний *час* пассажиров.

   <details>

   <summary>NYC Taxi: total passenger count</summary>

   ```sql
   INSERT INTO hw14.dashboards (dashboard, title, query)
   VALUES (
       'HW14 Dashboard',
       '[1] NYC Taxi: total passenger count',
   $dashboard_query$
   WITH
       {aggr_interval_sec:UInt32} as aggr_interval_sec,
       {history_depth_hr:UInt32} as history_depth_hr,
   {smoothing_interval_hr:UInt32} AS smoothing_interval_hr,
       t_interval_aggregated AS
       (
           SELECT
               toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND) AS time,
               sum(passenger_count) AS total_passengers
           FROM nyc_taxi.trips_small
           WHERE time >= (max_time - INTERVAL (history_depth_hr + smoothing_interval_hr) HOUR)
           GROUP BY time
           ORDER BY time ASC WITH FILL STEP aggr_interval_sec
       ),
       (
           SELECT max(toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND))
           FROM nyc_taxi.trips_small
       ) AS max_time
   SELECT
       time::INT AS t,
       sum(total_passengers) OVER (ORDER BY t ASC RANGE BETWEEN (smoothing_interval_hr * 3600) PRECEDING AND CURRENT ROW) AS moving_passenger_count
   FROM t_interval_aggregated
   WHERE time >= (max_time - INTERVAL history_depth_hr HOUR)
   $dashboard_query$
   );
   ```

   </details>

1. Среднее расстояние поездки за предыдущий *час*.

   <details>

   <summary>NYC Taxi: average trip distance</summary>

   ```sql
   INSERT INTO hw14.dashboards (dashboard, title, query)
   VALUES (
       'HW14 Dashboard',
       '[3] NYC Taxi: average trip distance',
   $dashboard_query$
   WITH
       {aggr_interval_sec:UInt32} as aggr_interval_sec,
       {history_depth_hr:UInt32} as history_depth_hr,
   {smoothing_interval_hr:UInt32} AS smoothing_interval_hr,
       t_interval_aggregated AS
       (
           SELECT
               toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND) AS time,
               avg(trip_distance) AS avg_distance
           FROM nyc_taxi.trips_small
           WHERE time >= (max_time - INTERVAL (history_depth_hr + smoothing_interval_hr) HOUR)
           GROUP BY time
           ORDER BY time ASC WITH FILL STEP aggr_interval_sec
       ),
       (
           SELECT max(toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND))
           FROM nyc_taxi.trips_small
       ) AS max_time
   SELECT
       time::INT AS t,
       avg(avg_distance) OVER (ORDER BY t ASC RANGE BETWEEN (smoothing_interval_hr * 3600) PRECEDING AND CURRENT ROW) AS moving_avg_trip_distance
   FROM t_interval_aggregated
   WHERE time >= (max_time - INTERVAL history_depth_hr HOUR)
   $dashboard_query$
   );
   ```

   </details>

1. Средний чек поездки за предыдущий *час*.

   <details>

   <summary>NYC Taxi: average fare amount</summary>

   ```sql
   INSERT INTO hw14.dashboards (dashboard, title, query)
   VALUES (
       'HW14 Dashboard',
       '[2] NYC Taxi: average fare amount',
   $dashboard_query$
   WITH
       {aggr_interval_sec:UInt32} as aggr_interval_sec,
       {history_depth_hr:UInt32} as history_depth_hr,
   {smoothing_interval_hr:UInt32} AS smoothing_interval_hr,
       t_interval_aggregated AS
       (
           SELECT
               toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND) AS time,
               avg(fare_amount) AS avg_fare
           FROM nyc_taxi.trips_small
           WHERE time >= (max_time - INTERVAL (history_depth_hr + smoothing_interval_hr) HOUR)
           GROUP BY time
           ORDER BY time ASC WITH FILL STEP aggr_interval_sec
       ),
       (
           SELECT max(toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND))
           FROM nyc_taxi.trips_small
       ) AS max_time
   SELECT
       time::INT AS t,
       avg(avg_fare) OVER (ORDER BY t ASC RANGE BETWEEN (smoothing_interval_hr * 3600) PRECEDING AND CURRENT ROW) AS moving_avg_fare_amount
   FROM t_interval_aggregated
   WHERE time >= (max_time - INTERVAL history_depth_hr HOUR)
   $dashboard_query$
   );
   ```

   </details>

1. Средний размер чаевых за предыдущий *час*.

   <details>

   <summary>NYC Taxi: average tip amount</summary>

   ```sql
   INSERT INTO hw14.dashboards (dashboard, title, query)
   VALUES (
       'HW14 Dashboard',
       '[4] NYC Taxi: average tip amount',
   $dashboard_query$
   WITH
       {aggr_interval_sec:UInt32} as aggr_interval_sec,
       {history_depth_hr:UInt32} as history_depth_hr,
   {smoothing_interval_hr:UInt32} AS smoothing_interval_hr,
       t_interval_aggregated AS
       (
           SELECT
               toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND) AS time,
               avg(tip_amount) AS avg_tip
           FROM nyc_taxi.trips_small
           WHERE time >= (max_time - INTERVAL (history_depth_hr + smoothing_interval_hr) HOUR)
           GROUP BY time
           ORDER BY time ASC WITH FILL STEP aggr_interval_sec
       ),
       (
           SELECT max(toStartOfInterval(pickup_datetime, INTERVAL aggr_interval_sec SECOND))
           FROM nyc_taxi.trips_small
       ) AS max_time
   SELECT
       time::INT AS t,
       avg(avg_tip) OVER (ORDER BY t ASC RANGE BETWEEN (smoothing_interval_hr * 3600) PRECEDING AND CURRENT ROW) AS moving_avg_tip_amount
   FROM t_interval_aggregated
   WHERE time >= (max_time - INTERVAL history_depth_hr HOUR)
   $dashboard_query$
   );

   ```

   </details>

Далее заходим через web-браузер на страницу встроенного дашборда *ClickHouse*, и в поле, где по умолчанию содержится имя стандартного дашборда (*"Overview"*), вводим текст запроса к нашей таблице с запросами:

```sql
SELECT title, query FROM hw14.dashboards WHERE dashboard IN ('HW14 Dashboard') ORDER BY title
```

Появившиеся в верхней левой части web-формы поля соответствуют параметрам наших запросов, их необходимо заполнить. Итоговый дашборд выглядит следующим образом:

![ClickHouse monitoring dashboards](/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-16_Monitoring/hw14/img/clickhouse-dashboard.png)

## Prometheus & Grafana

*Prometheus* и *Grafana* уже работают в своих контейнерах в одной виртуальной сети *Docker* с серверами кластера *ClickHouse* (я проводил настройку `docker-compose.yaml` по примерам из статьи [**(2)**](#biblio-02)). Также для *Prometheus* уже настроен источник *Node Exporter*, что позволяет очень быстро создать дашборд с системными метриками в *Grafana* (например, [11074](https://grafana.com/grafana/dashboards/15172-node-exporter-for-prometheus-dashboard-based-on-11074/)). [*Alertmanager*](https://prometheus.io/docs/alerting/latest/alertmanager/) я в этой работе не настраивал, потому что это не требовалось в задании.

### Prometheus

[Настроим](https://clickhouse.com/docs/operations/server-configuration-parameters/settings#prometheus) *ClickHouse* для экспорта метрик в *Prometheus*. Для этого в контейнере *ClickHouse* создадим файл [`/etc/clickhouse-server/config.d/prometheus.xml`](/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-16_Monitoring/hw14/cluster_1S_2R/fs/volumes/clickhouse-01/etc/clickhouse-server/config.d/prometheus.xml) со следующим содержимым:

```xml
<clickhouse>
    <prometheus>
        <endpoint>/metrics</endpoint>
        <port>9363</port>
        <metrics>true</metrics>
        <events>true</events>
        <asynchronous_metrics>true</asynchronous_metrics>
        <errors>true</errors>
    </prometheus>
</clickhouse>
```

После рестарта *ClickHouse* на потру 9363 станет доступен HTTP сервер, с которого *Prometheus* с заданной периодичностью будет собирать метрики сервера *ClickHouse*. Ниже пример того, в каком виде предоставляются метрики.

<details>

<summary>http://clickhouse-01:9363/metrics</summary>

```text
root@clickhouse-01:/# curl -s http://clickhouse-01:9363/metrics | head -50

# HELP ClickHouseProfileEvents_Query Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries.
# TYPE ClickHouseProfileEvents_Query counter
ClickHouseProfileEvents_Query 0
# HELP ClickHouseProfileEvents_SelectQuery Same as Query, but only for SELECT queries.
# TYPE ClickHouseProfileEvents_SelectQuery counter
ClickHouseProfileEvents_SelectQuery 0
# HELP ClickHouseProfileEvents_InsertQuery Same as Query, but only for INSERT queries.
# TYPE ClickHouseProfileEvents_InsertQuery counter
ClickHouseProfileEvents_InsertQuery 0
# HELP ClickHouseProfileEvents_InitialQuery Same as Query, but only counts initial queries (see is_initial_query).
# TYPE ClickHouseProfileEvents_InitialQuery counter
ClickHouseProfileEvents_InitialQuery 0
# HELP ClickHouseProfileEvents_QueriesWithSubqueries Count queries with all subqueries
# TYPE ClickHouseProfileEvents_QueriesWithSubqueries counter
ClickHouseProfileEvents_QueriesWithSubqueries 0
# HELP ClickHouseProfileEvents_SelectQueriesWithSubqueries Count SELECT queries with all subqueries
# TYPE ClickHouseProfileEvents_SelectQueriesWithSubqueries counter
ClickHouseProfileEvents_SelectQueriesWithSubqueries 0
# HELP ClickHouseProfileEvents_InsertQueriesWithSubqueries Count INSERT queries with all subqueries
# TYPE ClickHouseProfileEvents_InsertQueriesWithSubqueries counter
ClickHouseProfileEvents_InsertQueriesWithSubqueries 0
# HELP ClickHouseProfileEvents_SelectQueriesWithPrimaryKeyUsage Count SELECT queries which use the primary key to evaluate the WHERE condition
# TYPE ClickHouseProfileEvents_SelectQueriesWithPrimaryKeyUsage counter
ClickHouseProfileEvents_SelectQueriesWithPrimaryKeyUsage 0
# HELP ClickHouseProfileEvents_AsyncInsertQuery Same as InsertQuery, but only for asynchronous INSERT queries.
# TYPE ClickHouseProfileEvents_AsyncInsertQuery counter
ClickHouseProfileEvents_AsyncInsertQuery 0
# HELP ClickHouseProfileEvents_AsyncInsertBytes Data size in bytes of asynchronous INSERT queries.
# TYPE ClickHouseProfileEvents_AsyncInsertBytes counter
ClickHouseProfileEvents_AsyncInsertBytes 0
# HELP ClickHouseProfileEvents_AsyncInsertRows Number of rows inserted by asynchronous INSERT queries.
# TYPE ClickHouseProfileEvents_AsyncInsertRows counter
ClickHouseProfileEvents_AsyncInsertRows 0
# HELP ClickHouseProfileEvents_AsyncInsertCacheHits Number of times a duplicate hash id has been found in asynchronous INSERT hash id cache.
# TYPE ClickHouseProfileEvents_AsyncInsertCacheHits counter
ClickHouseProfileEvents_AsyncInsertCacheHits 0
# HELP ClickHouseProfileEvents_FailedQuery Number of failed queries.
# TYPE ClickHouseProfileEvents_FailedQuery counter
ClickHouseProfileEvents_FailedQuery 0
# HELP ClickHouseProfileEvents_FailedSelectQuery Same as FailedQuery, but only for SELECT queries.
# TYPE ClickHouseProfileEvents_FailedSelectQuery counter
ClickHouseProfileEvents_FailedSelectQuery 0
# HELP ClickHouseProfileEvents_FailedInsertQuery Same as FailedQuery, but only for INSERT queries.
# TYPE ClickHouseProfileEvents_FailedInsertQuery counter
ClickHouseProfileEvents_FailedInsertQuery 0
# HELP ClickHouseProfileEvents_FailedAsyncInsertQuery Number of failed ASYNC INSERT queries.
# TYPE ClickHouseProfileEvents_FailedAsyncInsertQuery counter
ClickHouseProfileEvents_FailedAsyncInsertQuery 0
# HELP ClickHouseProfileEvents_QueryTimeMicroseconds Total time of all queries.
# TYPE ClickHouseProfileEvents_QueryTimeMicroseconds counter
.  .  .  .  .
```

</details>

Теперь необходимо настроить *ClickHouse* как источник для *Prometheus*. Для этого в контейнере *Prometheus* необходимо добавить секцию в конфигурационный файл [`/etc/prometheus/prometheus.yml`](/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-16_Monitoring/hw14/cluster_1S_2R/fs/volumes/prometheus/prometheus.yml). Настроим сразу обе реплики *ClickHouse*:

```yaml
  - job_name: "clickhouse-01"
    static_configs:
      - targets: ["clickhouse-01:9363"]

  - job_name: "clickhouse-02"
    static_configs:
      - targets: ["clickhouse-02:9363"]
```

Источники в *Prometheus* теперь выглядят так:

![Prometheus targets](/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-16_Monitoring/hw14/img/prometheus-targets.png)

### Grafana

Далее остаётся только создать дашборд по метрикам *ClickHouse* в *Grafana*. Можно импортировать один из уже доступных дашбордов, например [14192](https://grafana.com/grafana/dashboards/14192-clickhouse/ "ClickHouse") или [23285](https://grafana.com/grafana/dashboards/23285-clickhouse-and-keeper-comprehensive-dashboard/ "ClickHouse and Keeper Comprehensive Dashboard"), которые рассматривались на лекции. Ниже пример того, как выглядит дашборд *ClickHouse* 14192 в *Grafana*.

![Grafana ClickHouse Dashboard 14192](/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-16_Monitoring/hw14/img/grafana-clickhouse-dashboard-14192.png)

## Репликация логов \*

### Таблица с движком [`Engine = Null`](https://clickhouse.com/docs/engines/table-engines/special/null)

Из задания не очень понятно, кто именно должен вставлять записи в таблицу с логами, поэтому я планирую просто вставить туда фрагмент из системной таблицы [`system.text_log`](https://clickhouse.com/docs/operations/system-tables/text_log). Соответственно, наша таблица с логами будет содержать подмножество колонок из *system.text_log*.

```sql
cluster_1S_2R node 1 :)

CREATE OR REPLACE TABLE hw14.logs_null
ENGINE = Null
AS SELECT
    event_date,
    event_time,
    level,
    message
FROM system.text_log

0 rows in set. Elapsed: 0.152 sec.
```

```sql
SHOW CREATE TABLE hw14.logs_null
FORMAT Raw

CREATE TABLE hw14.logs_null
(
    `event_date` Date,
    `event_time` DateTime,
    `level` Enum8('Fatal' = 1, 'Critical' = 2, 'Error' = 3, 'Warning' = 4, 'Notice' = 5, 'Information' = 6, 'Debug' = 7, 'Trace' = 8, 'Test' = 9),
    `message` String
)
ENGINE = Null

1 row in set. Elapsed: 0.008 sec.
```

### Реплицируемая таблица

Реплику будем идентифицировать просто по имени хоста, которое можно получить функцией [`hostName()`](https://clickhouse.com/docs/sql-reference/functions/other-functions#hostname).

```sql
cluster_1S_2R node 1 :)

CREATE TABLE hw14.logs_replicated
(
    `event_date` Date,
    `event_time` DateTime,
    `level` Enum8('Fatal' = 1, 'Critical' = 2, 'Error' = 3, 'Warning' = 4, 'Notice' = 5, 'Information' = 6, 'Debug' = 7, 'Trace' = 8, 'Test' = 9),
    `message` String,
    `replica` String MATERIALIZED hostName()
)
ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/{shard}/{database}/{table}', '{replica}')
PRIMARY KEY (event_date, event_time)

0 rows in set. Elapsed: 0.451 sec.
```

### Материализованное представление

```sql
cluster_1S_2R node 1 :)

CREATE MATERIALIZED VIEW hw14.logs_mv TO hw14.logs_replicated
AS SELECT
    event_date,
    event_time,
    level,
    message
FROM hw14.logs_null

0 rows in set. Elapsed: 0.042 sec.
```

Все предыдущие действия также повторим на втором узле кластера *ClickHouse*.

### Добавление данных

На узле *clickhouse-01* вставим в таблицу *hw14.logs_null* первые 5 записей из таблицы *system.text_log*.

```sql
cluster_1S_2R node 1 :)

INSERT INTO hw14.logs_null SELECT
    event_date,
    event_time,
    level,
    message
FROM system.text_log
LIMIT 5

0 rows in set. Elapsed: 0.100 sec.
```

А на узле *clickhouse-02* вставим в таблицу *hw14.logs_null* последние на момент выполнения команды 5 записей из таблицы *system.text_log*.

```sql
cluster_1S_2R node 2 :)

INSERT INTO hw14.logs_null WITH (
        SELECT count()
        FROM system.text_log
    ) AS cnt
SELECT
    event_date,
    event_time,
    level,
    message
FROM system.text_log
LIMIT 5 OFFSET cnt - 5

0 rows in set. Elapsed: 0.098 sec. Processed 122.66 thousand rows, 12.26 MB (1.26 million rows/s., 125.51 MB/s.)
Peak memory usage: 4.90 MiB.
```

Посмотрим на итоговое содержимое таблицы *hw14.logs_replicated* на узле *clickhouse-01*:

```sql
cluster_1S_2R node 1 :)

SELECT
    replica,
    event_date,
    event_time,
    level,
    concat(substring(message, 1, 64), multiIf(length(message) > 64, '...', '')) AS message_head
FROM hw14.logs_replicated

    ┌─replica───────┬─event_date─┬──────────event_time─┬─level───────┬─message_head────────────────────────────────────────────────────────┐
 1. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Debug       │ Sending crash reports is initialized with https://crash.clickhou... │
 2. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Debug       │ Sending logical errors is enabled                                   │
 3. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ Starting ClickHouse 25.6.4.12 (revision: 54502, git hash: c3c91c... │
 4. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ starting up                                                         │
 5. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ OS name: Linux, version: 5.14.0-596.el9.x86_64, architecture: x8... │
 6. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:49 │ Debug       │ Merge sorted 1821958 rows, containing 5 columns (5 merged, 0 gat... │
 7. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:49 │ Debug       │ Background process (mutate/merge) peak memory usage: 17.31 MiB.     │
 8. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removing 6 parts from filesystem (serially): Parts: [202508_3096... │
 9. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removing 6 parts from memory: Parts: [202508_3096_3141_9, 202508... │
10. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removed 6 old parts                                                 │
    └───────────────┴────────────┴─────────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────┘

10 rows in set. Elapsed: 0.017 sec.
```

И на узле *clickhouse-02*:

```sql
cluster_1S_2R node 2 :)

SELECT
    replica,
    event_date,
    event_time,
    level,
    concat(substring(message, 1, 64), multiIf(length(message) > 64, '...', '')) AS message_head
FROM hw14.logs_replicated

    ┌─replica───────┬─event_date─┬──────────event_time─┬─level───────┬─message_head────────────────────────────────────────────────────────┐
 1. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Debug       │ Sending crash reports is initialized with https://crash.clickhou... │
 2. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Debug       │ Sending logical errors is enabled                                   │
 3. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ Starting ClickHouse 25.6.4.12 (revision: 54502, git hash: c3c91c... │
 4. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ starting up                                                         │
 5. │ clickhouse-01 │ 2025-08-04 │ 2025-08-04 12:23:53 │ Information │ OS name: Linux, version: 5.14.0-596.el9.x86_64, architecture: x8... │
 6. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:49 │ Debug       │ Merge sorted 1821958 rows, containing 5 columns (5 merged, 0 gat... │
 7. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:49 │ Debug       │ Background process (mutate/merge) peak memory usage: 17.31 MiB.     │
 8. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removing 6 parts from filesystem (serially): Parts: [202508_3096... │
 9. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removing 6 parts from memory: Parts: [202508_3096_3141_9, 202508... │
10. │ clickhouse-02 │ 2025-08-04 │ 2025-08-04 19:27:50 │ Debug       │ Removed 6 old parts                                                 │
    └───────────────┴────────────┴─────────────────────┴─────────────┴─────────────────────────────────────────────────────────────────────┘

10 rows in set. Elapsed: 0.005 sec.
```

Можно видеть, что содержимое таблиц одинаковое, включая поле, идентифицирующее реплику. Интересный момент в том, что, как мы видим, значение [материализованной колонки](https://clickhouse.com/docs/sql-reference/statements/alter/column#materialize-column) вычисляется один раз на узле, на котором происходит вставка, и затем копируется на остальные реплики (то есть уже не вычисляется на них). Очевидно, эта логика была реализована для поддержания согласованности данных между репликами.

Наконец, отметим, что таблица *hw14.logs_null* никаких данных, естественно, не содержит.

```sql
SELECT *
FROM hw14.logs_null

0 rows in set. Elapsed: 0.005 sec.
```

## Библиография

<a name="biblio-01"></a>
**\[1]** [*Common issues you can solve using advanced monitoring dashboards*](https://clickhouse.com/blog/common-issues-you-can-solve-using-advanced-monitoring-dashboards "clickhouse.com/blog/")

<a name="biblio-02"></a>
**\[2]** [*Prometheus with Docker Compose: The Complete Setup Guide*](https://last9.io/blog/prometheus-with-docker-compose/ "last9.io/blog")

<a name="biblio-03"></a>
**\[3]** [*A Prometheus & Grafana docker-compose stack*](https://github.com/vegasbrianc/prometheus "github.com/vegasbrianc/prometheus")

<a name="biblio-04"></a>
**\[4]** [*Enhancing ClickHouse Query Efficiency: The Power of Materialized Columns in Practice*](https://chistadata.com/clickhouse-query-efficiency-materialized-columns/ "chistadata.com")
