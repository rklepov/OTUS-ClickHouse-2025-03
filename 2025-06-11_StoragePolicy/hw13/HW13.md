# [13] Storage Policy и резервное копирование

> Я работаю локально через [VirtualBox](https://www.virtualbox.org/) на Linux [CentOS 9](https://www.centos.org/stream9/).

## Архитектура решения

Общий план решения следующий:

1. Хранилище *S3* будет развернуто с помощью [*MinIO*](https://min.io/docs/minio/container/index.html) в [контейнере](https://quay.io/repository/minio/minio) *Docker*.

2. Будем запускать 2 сервера (*ClickHouse* и *S3*) в [виртуальной сети](https://docs.docker.com/compose/how-tos/networking/) `docker compose`.

   <details>

   <summary>docker-compose.yaml</summary>

   ```yaml
   version: "3.8"
   services:
   clickhouse-01:
      image: "clickhouse/clickhouse-server:${CHVER:-latest}"
      user: "0:0"
      container_name: clickhouse-01
      networks:
         - net
      hostname: clickhouse-01
      volumes:
         - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml:Z
         - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml:Z
         - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-backup/config.yml:/etc/clickhouse-backup/config.yml:Z
      ports:
         - "0.0.0.0:18123:8123"
         - "0.0.0.0:19000:9000"
      depends_on:
         - minio-s3

   minio-s3:
      image: "quay.io/minio/minio"
      container_name: minio-s3
      networks:
         - net
      hostname: minio-s3
      command: ["server", "/data", "--console-address", ":9001"]
      volumes:
         - s3:/data:Z
      ports:
         - "0.0.0.0:19090:9000"
         - "0.0.0.0:19091:9001"
      environment:
         - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minio}
         - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-miniosecret}

   networks:
   net:
      driver: bridge

   volumes:
   s3:
   ```

   </details>

3. Утилита `clickhouse-backup` будет установлена непосредственно в контейнер с сервером *ClickHouse* вручную.

Конфигурационные файлы решения целиком можно найти в текущем [репозитории](https://github.com/rklepov/OTUS-ClickHouse-2025-03/tree/main/2025-06-11_StoragePolicy/hw13).

Запустим контейнеры с помощью команды `docker compose up -d`.

```shell
$ docker ps

CONTAINER ID  IMAGE                                          COMMAND               CREATED             STATUS             PORTS                                                       NAMES
ebbf3fbd4611  quay.io/minio/minio:latest                     server /data --co...  About a minute ago  Up About a minute  0.0.0.0:19090-19091->9000-9001/tcp                          minio-s3
e061f0ac21a3  docker.io/clickhouse/clickhouse-server:latest                        About a minute ago  Up About a minute  0.0.0.0:18123->8123/tcp, 0.0.0.0:19000->9000/tcp, 9009/tcp  clickhouse-01
```

## Развёртывание *S3*

Настроим ключи доступа к локальному хранилищу *S3* внутри контейнера *MinIO* с помощью утилиты `mс` ([*MinIO Client*](https://min.io/docs/minio/linux/reference/minio-mc.html)) в соответствии с [инструкцией](https://min.io/docs/minio/container/index.html#procedure). Также создадим *S3 бакет* **clickhouse-backup**, в который в дальнейшем будем загружать резервную копию.

```shell
$ docker exec -it minio-s3 bash

bash-5.1# mc alias set local http://127.0.0.1:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}
mc: Configuration written to `/tmp/.mc/config.json`. Please update your access credentials.
mc: Successfully created `/tmp/.mc/share`.
mc: Initialized share uploads `/tmp/.mc/share/uploads.json` file.
mc: Initialized share downloads `/tmp/.mc/share/downloads.json` file.
Added `local` successfully.

bash-5.1# mc alias ls local
local
  URL       : http://127.0.0.1:9000
  AccessKey : minio
  SecretKey : miniosecret
  API       : s3v4
  Path      : auto
  Src       : /tmp/.mc/config.json

bash-5.1# mc mb local/clickhouse-backup
Bucket created successfully `local/clickhouse-backup`.
```

## Установка `clickhouse-backup`

Утилиту [`clickhouse-backup`](https://github.com/Altinity/clickhouse-backup) необходимо устанавливать непосредственно на сервер с *ClickHouse* и запускать под тем же пользователем, что и процесс `clickhouse-server`, потому что ей необходим прямой доступ к файловой системе (к содержимому `/var/lib/clickhouse`).

Установим утилиту `clickhouse-backup` в соответствии с инструкцией с занятия. Контейнер *clickhouse-01* настроен на запуск под **root**, поэтому в данном случае нет необходимости в [`sudo`](https://man7.org/linux/man-pages/man8/sudo.8.html).

```shell
$ docker exec -it clickhouse-01 bash

root@clickhouse-01:/# mkdir pkg && cd pkg

root@clickhouse-01:/pkg# wget -q wget https://github.com/Altinity/clickhouse-backup/releases/latest/download/clickhouse-backup-linux-amd64.tar.gz

root@clickhouse-01:/pkg# tar xf clickhouse-backup-linux-amd64.tar.gz

root@clickhouse-01:/pkg# install -o root -g root -m 0755 build/linux/amd64/clickhouse-backup /usr/local/bin

root@clickhouse-01:/pkg# clickhouse-backup -v
Version:         2.6.23
Git Commit:      a9f9ce80bd355c209acfa96166a85a2ac269899c
Build Date:      2025-06-18
```

Настройки `clickhouse-backup` содержатся в файле `/etc/clickhouse-backup/config.yml` (он также доступен в текущем [репозитории](https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-11_StoragePolicy/hw13/fs/volumes/clickhouse-01/etc/clickhouse-backup/config.yml)).

<details>

<summary>/etc/clickhouse-backup/config.yml</summary>

```yaml
general:
    remote_storage: s3
    max_file_size: 0
    backups_to_keep_local: 0
    backups_to_keep_remote: 0
    log_level: warning
    allow_empty_backups: false
    download_concurrency: 2
    upload_concurrency: 1
    upload_max_bytes_per_second: 0
    download_max_bytes_per_second: 0
    object_disk_server_side_copy_concurrency: 32
    allow_object_disk_streaming: false
    use_resumable_state: true
    restore_schema_on_cluster: ""
    upload_by_part: true
    download_by_part: true
    restore_database_mapping: {}
    restore_table_mapping: {}
    retries_on_failure: 3
    retries_pause: 5s
    watch_interval: 1h
    full_interval: 24h
    watch_backup_name_template: shard{shard}-{type}-{time:20060102150405}
    sharded_operation_mode: ""
    cpu_nice_priority: 15
    io_nice_priority: idle
    rbac_backup_always: true
    rbac_conflict_resolution: recreate
    retriesduration: 5s
    watchduration: 1h0m0s
    fullduration: 24h0m0s
clickhouse:
    username: default
    password: ""
    host: clickhouse-01
    port: 9000
    disk_mapping: {}
    skip_tables:
        - system.*
        - INFORMATION_SCHEMA.*
        - information_schema.*
        - _temporary_and_external_tables.*
    skip_table_engines: []
    skip_disks: []
    skip_disk_types: []
    timeout: 30m
    freeze_by_part: false
    freeze_by_part_where: ""
    use_embedded_backup_restore: false
    embedded_backup_disk: ""
    backup_mutations: true
    restore_as_attach: false
    check_parts_columns: true
    secure: false
    skip_verify: false
    sync_replicated_tables: false
    log_sql_queries: true
    config_dir: /etc/clickhouse-server/
    restart_command: exec:systemctl restart clickhouse-server
    ignore_not_exists_error_during_freeze: true
    check_replicas_before_attach: true
    default_replica_path: /clickhouse/tables/{cluster}/{shard}/{database}/{table}
    default_replica_name: '{replica}'
    tls_key: ""
    tls_cert: ""
    tls_ca: ""
    max_connections: 2
    debug: false
s3:
    access_key: "minio"
    secret_key: "miniosecret"
    bucket: clickhouse-backup
    endpoint: http://minio-s3:9000
    region: "xyz"
    acl: private
    assume_role_arn: ""
    force_path_style: true
    path: ""
    object_disk_path: ""
    disable_ssl: true
    compression_level: 1
    compression_format: tar
    sse: ""
    sse_kms_key_id: ""
    sse_customer_algorithm: ""
    sse_customer_key: ""
    sse_customer_key_md5: ""
    sse_kms_encryption_context: ""
    disable_cert_verification: false
    use_custom_storage_class: false
    storage_class: STANDARD
    custom_storage_class_map: {}
    concurrency: 3
    max_parts_count: 4000
    allow_multipart_download: false
    object_labels: {}
    request_payer: ""
    check_sum_algorithm: ""
    retry_mode: standard
    debug: false
```

</details>

> В данном пункте задания я не совсем понял смысл указания *"настройте политику хранения (storage policy) в конфигурации ClickHouse"*. `clickhouse-backup` не использует политики хранения. В последующих частях задания они тоже не требуются. Поэтому данное действие я опустил.

## Тестовая база данных

В данном задании нас в первую очередь интересует сама схема резервного копирования данных в &laquo;холодное&raquo; хранилище. С этой точки зрения содержимое таблиц не так важно. Поэтому я решил создать 3 простых таблицы с минимальным количеством записей, чтобы проверить 3 варианта &laquo;повреждения&raquo; данных:

1. Удаление таблицы.
2. Удаление колонки.
3. Удаление записи в таблице.

Создание таблиц:

```sql
CREATE DATABASE IF NOT EXISTS hw13;

CREATE OR REPLACE TABLE hw13.table1
(
    `d` DateTime64,
    `n` Int64,
    `u` String
)
ENGINE = MergeTree
ORDER BY d;

CREATE OR REPLACE TABLE hw13.table2 AS hw13.table1;

CREATE OR REPLACE TABLE hw13.table3 AS hw13.table1;
```

Заполнение тестовыми данными:

```sql
clickhouse-01 :) insert into hw13.table1 values (now(), 1, 'abc'),(now(), 2, 'def'),(now(), 3, 'ghi');

SELECT *
FROM hw13.table1

   ┌───────────────────────d─┬─n─┬─u───┐
1. │ 2025-06-20 21:25:00.000 │ 1 │ abc │
2. │ 2025-06-20 21:25:00.000 │ 2 │ def │
3. │ 2025-06-20 21:25:00.000 │ 3 │ ghi │
   └─────────────────────────┴───┴─────┘

3 rows in set. Elapsed: 0.004 sec.


clickhouse-01 :) insert into hw13.table2 values (now(), 11, 'jkl'),(now(), 12, 'mno'),(now(), 13, 'pqr');

SELECT *
FROM hw13.table2

   ┌───────────────────────d─┬──n─┬─u───┐
1. │ 2025-06-20 21:25:05.000 │ 11 │ jkl │
2. │ 2025-06-20 21:25:05.000 │ 12 │ mno │
3. │ 2025-06-20 21:25:05.000 │ 13 │ pqr │
   └─────────────────────────┴────┴─────┘

3 rows in set. Elapsed: 0.004 sec.


clickhouse-01 :) insert into hw13.table3 values (now(), 21, 'stu'),(now(), 22, 'vwx'),(now(), 23, 'yz.');

SELECT *
FROM hw13.table3

   ┌───────────────────────d─┬──n─┬─u───┐
1. │ 2025-06-20 21:25:10.000 │ 21 │ stu │
2. │ 2025-06-20 21:25:10.000 │ 22 │ vwx │
3. │ 2025-06-20 21:25:10.000 │ 23 │ yz. │
   └─────────────────────────┴────┴─────┘

3 rows in set. Elapsed: 0.003 sec.


SELECT
    database,
    name
FROM system.tables
WHERE database IN ('hw13')

   ┌─database─┬─name───┐
1. │ hw13     │ table1 │
2. │ hw13     │ table2 │
3. │ hw13     │ table3 │
   └──────────┴────────┘

3 rows in set. Elapsed: 0.004 sec.
```

## Резервное копирование

Создаём локальную резервную копию наших таблиц и проверяем содержимое `/var/lib/clickhouse/backup`.

```shell
root@clickhouse-01:/# clickhouse-backup create -t "hw13.*" mybackup

root@clickhouse-01:/# clickhouse-backup list local
mybackup   20/06/2025 21:28:18   local       all:3.42KiB,data:2.31KiB,arch:0B,obj:1.11KiB,meta:0B,rbac:0B,conf:0B          regular

root@clickhouse-01:/# ls -hAlF /var/lib/clickhouse/backup/mybackup/shadow/hw13/
total 0
drwxr-x---. 3 clickhouse clickhouse 21 Jun 20 21:28 table1/
drwxr-x---. 3 clickhouse clickhouse 21 Jun 20 21:28 table2/
drwxr-x---. 3 clickhouse clickhouse 21 Jun 20 21:28 table3/
```

Загружаем резервную копию в хранилище *S3*.

```shell
root@clickhouse-01:/# clickhouse-backup upload mybackup

root@clickhouse-01:/# clickhouse-backup list all
mybackup   20/06/2025 21:28:18   local       all:3.42KiB,data:2.31KiB,arch:0B,obj:1.11KiB,meta:0B,rbac:0B,conf:0B          regular
mybackup   20/06/2025 21:28:29   remote      all:28.29KiB,data:2.31KiB,arch:27.00KiB,obj:0B,meta:1.29KiB,rbac:0B,conf:0B   tar, regular
```

Проверим, как выглядят загруженные данные через веб-интерфейс *консоли MinIO*.

![minio_console][minio_console]

Посмотрим на те же данные в файловой системе контейнера *minio-s3* в выводе утилиты `mc`.

```shell
bash-5.1# mc ls local/clickhouse-backup/mybackup/shadow/hw13
[2025-06-20 21:36:38 UTC]     0B table1/
[2025-06-20 21:36:38 UTC]     0B table2/
[2025-06-20 21:36:38 UTC]     0B table3/

bash-5.1# mc ls local/clickhouse-backup/mybackup/shadow/hw13/table1
[2025-06-20 21:28:29 UTC] 9.0KiB STANDARD default_all_1_1_0.tar
```

Поскольку backup был успешно загружен в хранилище *S3*, то локальную копию можно удалить.

```shell
root@clickhouse-01:/# clickhouse-backup delete local mybackup

root@clickhouse-01:/# clickhouse-backup list all
mybackup   20/06/2025 21:28:29   remote      all:28.29KiB,data:2.31KiB,arch:27.00KiB,obj:0B,meta:1.29KiB,rbac:0B,conf:0B   tar, regular
```

[minio_console]: https://github.com/rklepov/OTUS-ClickHouse-2025-03/blob/main/2025-06-11_StoragePolicy/hw13/minio_console.png

## &laquo;Повреждение&raquo; данных в БД

Первую таблицу удалим полностью.

```sql
DROP TABLE hw13.table1 SYNC

0 rows in set. Elapsed: 0.004 sec.


SELECT
    database,
    name
FROM system.tables
WHERE database IN ('hw13')

   ┌─database─┬─name───┐
1. │ hw13     │ table2 │
2. │ hw13     │ table3 │
   └──────────┴────────┘

2 rows in set. Elapsed: 0.004 sec.
```

Во второй таблице удалим одну колонку.

```sql
ALTER TABLE hw13.table2
    (DROP COLUMN n)

0 rows in set. Elapsed: 0.021 sec.


SELECT *
FROM hw13.table2

   ┌───────────────────────d─┬─u───┐
1. │ 2025-06-20 21:25:05.000 │ jkl │
2. │ 2025-06-20 21:25:05.000 │ mno │
3. │ 2025-06-20 21:25:05.000 │ pqr │
   └─────────────────────────┴─────┘

3 rows in set. Elapsed: 0.003 sec.
```

В третьей таблице удалим одну запись.

```sql
ALTER TABLE hw13.table3
    (DELETE WHERE n = 22)

0 rows in set. Elapsed: 0.011 sec.


SELECT *
FROM hw13.table3

   ┌───────────────────────d─┬──n─┬─u───┐
1. │ 2025-06-20 21:25:10.000 │ 21 │ stu │
2. │ 2025-06-20 21:25:10.000 │ 23 │ yz. │
   └─────────────────────────┴────┴─────┘

2 rows in set. Elapsed: 0.003 sec.
```

## Восстановление данных из резервной копии

Загрузим резервную копию из *S3* и восстановим данные.

```shell
root@clickhouse-01:/# clickhouse-backup download mybackup

root@clickhouse-01:/# clickhouse-backup list local
mybackup   20/06/2025 21:28:18   local      all:3.60KiB,data:2.31KiB,arch:0B,obj:1.29KiB,meta:0B,rbac:0B,conf:0B   regular

root@clickhouse-01:/# clickhouse-backup restore mybackup
```

Убедимся, что данные были успешно восстановлены, повторив запрос на наши таблицы:

```sql
SELECT
    database,
    name
FROM system.tables
WHERE database IN ('hw13')

   ┌─database─┬─name───┐
1. │ hw13     │ table1 │
2. │ hw13     │ table2 │
3. │ hw13     │ table3 │
   └──────────┴────────┘

3 rows in set. Elapsed: 0.004 sec.


SELECT *
FROM hw13.table1

   ┌───────────────────────d─┬─n─┬─u───┐
1. │ 2025-06-20 21:25:00.000 │ 1 │ abc │
2. │ 2025-06-20 21:25:00.000 │ 2 │ def │
3. │ 2025-06-20 21:25:00.000 │ 3 │ ghi │
   └─────────────────────────┴───┴─────┘

3 rows in set. Elapsed: 0.010 sec.


SELECT *
FROM hw13.table2

   ┌───────────────────────d─┬──n─┬─u───┐
1. │ 2025-06-20 21:25:05.000 │ 11 │ jkl │
2. │ 2025-06-20 21:25:05.000 │ 12 │ mno │
3. │ 2025-06-20 21:25:05.000 │ 13 │ pqr │
   └─────────────────────────┴────┴─────┘

3 rows in set. Elapsed: 0.005 sec.


SELECT *
FROM hw13.table3

   ┌───────────────────────d─┬──n─┬─u───┐
1. │ 2025-06-20 21:25:10.000 │ 21 │ stu │
2. │ 2025-06-20 21:25:10.000 │ 22 │ vwx │
3. │ 2025-06-20 21:25:10.000 │ 23 │ yz. │
   └─────────────────────────┴────┴─────┘

3 rows in set. Elapsed: 0.004 sec.
```

Как можно убедиться, данные в таблицах приняли первоначальный вид.
